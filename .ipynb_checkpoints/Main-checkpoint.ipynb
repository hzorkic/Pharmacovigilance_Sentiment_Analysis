{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c86f5fe",
   "metadata": {},
   "source": [
    "# Using Sentiment Analysis for Pharmacovigilance of Contraceptives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538764c",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "In order to gather testing data, we need to collect data from the website Drugs.com. Scraping data from the web simply means programatically collecting specific data using raw html content from websites. A common library for webscraping in python is called Beautiful Soup. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a42bd",
   "metadata": {},
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bce0fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14478488",
   "metadata": {},
   "source": [
    "**Load out first page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf0ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   HTML Example\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <div align=\"middle\">\n",
      "   <h1>\n",
      "    HTML Webpage\n",
      "   </h1>\n",
      "   <p>\n",
      "    Link to more interesting example:\n",
      "    <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">\n",
      "     keithgalli.github.io/web-scraping/webpage.html\n",
      "    </a>\n",
      "   </p>\n",
      "  </div>\n",
      "  <h2>\n",
      "   A Header\n",
      "  </h2>\n",
      "  <p>\n",
      "   <i>\n",
      "    Some italicized text\n",
      "   </i>\n",
      "  </p>\n",
      "  <h2>\n",
      "   Another header\n",
      "  </h2>\n",
      "  <p id=\"paragraph-id\">\n",
      "   <b>\n",
      "    Some bold text\n",
      "   </b>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load webpage content\n",
    "r = requests.get(\"https://keithgalli.github.io/web-scraping/example.html\")\n",
    "\n",
    "# Covnert to a beautiful soup object\n",
    "soup = bs(r.content)\n",
    "\n",
    "# print out our html\n",
    "print(soup.prettify())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35525d24",
   "metadata": {},
   "source": [
    "**Start using beautiful soup**\n",
    "\n",
    "The find command finds the first element of a specific html type. \n",
    "The find_all command finds all elements of a specific html type. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29cc584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>A Header</h2>\n",
      "[<h2>A Header</h2>, <h2>Another header</h2>]\n"
     ]
    }
   ],
   "source": [
    "first_header = soup.find(\"h2\")\n",
    "print(first_header)\n",
    "\n",
    "headers = soup.find_all(\"h2\")\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7bd006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>HTML Webpage</h1>\n",
      "[<h1>HTML Webpage</h1>, <h2>A Header</h2>, <h2>Another header</h2>]\n"
     ]
    }
   ],
   "source": [
    "# pass in a list of elements to look for\n",
    "\n",
    "# find is gonna find the first occurance of the first item in the list\n",
    "first_header = soup.find([\"h1\", \"h2\"])\n",
    "print(first_header)\n",
    "\n",
    "# find_all is gonna find all occurances of both items in the list\n",
    "headers = soup.find_all([\"h1\",\"h2\"])\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9f5a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>, <p><i>Some italicized text</i></p>, <p id=\"paragraph-id\"><b>Some bold text</b></p>]\n",
      "[<p id=\"paragraph-id\"><b>Some bold text</b></p>]\n"
     ]
    }
   ],
   "source": [
    "# pass in attributed to the find / find_all function\n",
    "paragraph = soup.find_all(\"p\")\n",
    "print(paragraph)\n",
    "\n",
    "# find the paragraph with specific a paragraph id\n",
    "paragraph = soup.find_all(\"p\", attrs={\"id\":\"paragraph-id\"})\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652c7ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\n",
      "div\n"
     ]
    }
   ],
   "source": [
    "# we can nest find and find_all calls\n",
    "body = soup.find(\"body\")\n",
    "print(\"body\")\n",
    "# look for a div within the body \n",
    "div = body.find(\"div\")\n",
    "print(\"div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bba7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p><i>Some italicized text</i></p>, <p id=\"paragraph-id\"><b>Some bold text</b></p>]\n",
      "[<h2>A Header</h2>, <h2>Another header</h2>]\n"
     ]
    }
   ],
   "source": [
    "# we can search for specific strings within find / find_all calls\n",
    "import re\n",
    "\n",
    "# find any paragraph with the string \"Some\"\n",
    "paragraphs = soup.find_all(\"p\", string=re.compile(\"Some\"))\n",
    "print(paragraphs)\n",
    "\n",
    "# find any header with the string \"header or Header\"\n",
    "headers = soup.find_all(\"h2\", string=re.compile(\"(H|h)eader\"))\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662936ad",
   "metadata": {},
   "source": [
    "**Select (CSS selector)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d676023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>]\n"
     ]
    }
   ],
   "source": [
    "# select is very similar to find_all\n",
    "content = soup.select(\"div p\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85601dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p><i>Some italicized text</i></p>,\n",
       " <p id=\"paragraph-id\"><b>Some bold text</b></p>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = soup.select(\"h2 ~ p\")\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a6156dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<b>Some bold text</b>]\n"
     ]
    }
   ],
   "source": [
    "# grab the bold text element after a paragrpah with an id \"paragraph id\"\n",
    "bold_text = soup.select(\"p#paragraph-id b\")\n",
    "print(bold_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4289139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p><i>Some italicized text</i></p>, <p id=\"paragraph-id\"><b>Some bold text</b></p>]\n",
      "[<i>Some italicized text</i>]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# select bodies with a paragraph direct descendent \n",
    "paragraphs = soup.select(\"body > p\")\n",
    "print(paragraphs)\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph.select(\"i\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ada9f9",
   "metadata": {},
   "source": [
    "**Getting different properties of the html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055e822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Header\n",
      "<div align=\"middle\">\n",
      " <h1>\n",
      "  HTML Webpage\n",
      " </h1>\n",
      " <p>\n",
      "  Link to more interesting example:\n",
      "  <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">\n",
      "   keithgalli.github.io/web-scraping/webpage.html\n",
      "  </a>\n",
      " </p>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets get just the TEXT not the full element\n",
    "header = soup.find(\"h2\")\n",
    "print(header.string)\n",
    "\n",
    "div = soup.find(\"div\")\n",
    "print(div.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749575e",
   "metadata": {},
   "source": [
    "**Get HTML Webpage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea232ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paragraph-id'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a specific property from an element\n",
    "link = soup.find(\"a\")\n",
    "link['href']\n",
    "\n",
    "paragraphs = soup.select(\"p#paragraph-id\")\n",
    "paragraphs[0][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e414a87",
   "metadata": {},
   "source": [
    "**Code navigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8588b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      " <div align=\"middle\">\n",
      "  <h1>\n",
      "   HTML Webpage\n",
      "  </h1>\n",
      "  <p>\n",
      "   Link to more interesting example:\n",
      "   <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">\n",
      "    keithgalli.github.io/web-scraping/webpage.html\n",
      "   </a>\n",
      "  </p>\n",
      " </div>\n",
      " <h2>\n",
      "  A Header\n",
      " </h2>\n",
      " <p>\n",
      "  <i>\n",
      "   Some italicized text\n",
      "  </i>\n",
      " </p>\n",
      " <h2>\n",
      "  Another header\n",
      " </h2>\n",
      " <p id=\"paragraph-id\">\n",
      "  <b>\n",
      "   Some bold text\n",
      "  </b>\n",
      " </p>\n",
      "</body>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path systems\n",
    "print(soup.body.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aba4ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2>A Header</h2>,\n",
       " <p><i>Some italicized text</i></p>,\n",
       " <h2>Another header</h2>,\n",
       " <p id=\"paragraph-id\"><b>Some bold text</b></p>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# know the terms parent, sibling, child\n",
    "# elements on the same level are siblings\n",
    "soup.body.find(\"div\").find_next_siblings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d143ad",
   "metadata": {},
   "source": [
    "## Scrape drug reviews from Drugs.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe11b53",
   "metadata": {},
   "source": [
    "We create a dataframe with the following format:\n",
    "\n",
    "#### | drug name | review | rating |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34146d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. Get data from the \"contraceptives\" page on drugs.com\n",
    "    url = \"https://www.drugs.com/drug-class/contraceptives.html\"\n",
    "    r, webpage = get_webpage(url)\n",
    "\n",
    "    # 2. Get table with drug names, reviews, etc.\n",
    "    table_body = webpage.find(\"table\", {\"class\":\"ddc-table-sortable\"})\n",
    "    # create an empty dataframe\n",
    "    drug_data = pd.DataFrame(columns=['date', 'sentiment', 'review'])\n",
    "    \n",
    "    # loop through the table with drug names\n",
    "    for row in table_body.find_all(\"tr\")[1:-1]:\n",
    "        # get the drug name\n",
    "        drug_name = row.td.a.b.string\n",
    "        \n",
    "    \n",
    "        # get the number of reviews\n",
    "        if row.find(\"a\", {\"class\":\"ddc-text-nowrap\"}):\n",
    "            # grab the number and convert the string into an integer\n",
    "            drug_reviews = row.find(\"a\", {\"class\":\"ddc-text-nowrap\"}).string.split(' ')[0]\n",
    "            drug_reviews = int(drug_reviews.replace(',', ''))\n",
    "        \n",
    "            url_ = \"https://www.drugs.com\" + str(row.find(\"a\", {\"class\":\"ddc-text-nowrap\"}, href=True)['href'])\n",
    "            # if there are more than 100 reviews, we add it to the dataframe\n",
    "            if drug_reviews >= 100:\n",
    "                # get the reviews, dates, and sentiments of all drug reviews posted after 2021\n",
    "                reviews = get_reviews(url_, drug_name, drug_reviews, '2021')\n",
    "            \n",
    "    return reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00122ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpage(link):\n",
    "    '''Get the contents of a webpage.\n",
    "    Input: \n",
    "        link = Link to the desired webpage\n",
    "    Output: Beautiufl Soup Object containing the HTML data\n",
    "    '''\n",
    "    # load the webpage content \n",
    "    r = requests.get(link)\n",
    "    # convert to a beautiful soup object \n",
    "    webpage = bs(r.content)\n",
    "    return r, webpage\n",
    "\n",
    "\n",
    "def get_reviews(url_, drug_name, drug_reviews, date_cutoff=False):\n",
    "    '''Get all of the reviews of a specific drug\n",
    "    Input: \n",
    "        drug_name = name of the drug\n",
    "        drug_reviews = number of reviews\n",
    "        date_cutoff = Do you want to limit the date of reviews collected?\n",
    "    Output: A list containing a list for each review: [str('Review goes here'), int(rating)]\n",
    "    '''\n",
    "    #------------------------#\n",
    "    # 1. initalize variables #\n",
    "    #------------------------#\n",
    "    reviews, isHaveNextPage, page = [], True, 0\n",
    "    \n",
    "    #---------------------------------------------------------#\n",
    "    # 2. \"clicking\" the \"#### Reviews\" hyperlink in the table #\n",
    "    #---------------------------------------------------------#\n",
    "    review_page_url = url_\n",
    "    \n",
    "    #---------------------------------------#\n",
    "    # 3. cycle through each page of reviews #\n",
    "    #---------------------------------------#\n",
    "    while isHaveNextPage: \n",
    "        \n",
    "        # access the page and sort the reviews by most recent reviews\n",
    "        r_, review_page_content = get_webpage(review_page_url + f\"?sort_reviews=most_recent&page={page}\")\n",
    "        \n",
    "        # TO-DO: There is an issue here with grabbing reviews after page 5\n",
    "        list_of_review_boxes = review_page_content.find_all(\"div\", {\"class\":\"ddc-comment ddc-box ddc-mgb-2\"})\n",
    "        \n",
    "        # grab the date, review paragraph, and rating\n",
    "        for review in list_of_review_boxes:\n",
    "            # 1. find the date, if it is at the cutoff year, ignore\n",
    "            head = review.find(\"div\", {\"class\":\"ddc-comment-header\"})\n",
    "            date = head.find(\"span\", string=re.compile(\", \")).string\n",
    "            if date.endswith(date_cutoff):\n",
    "                return reviews\n",
    "            \n",
    "            # 3. get the review paragraph\n",
    "            if review.p.b:\n",
    "                review_paragraph = str(review.p.b.next_sibling).strip()[1:-1]\n",
    "            else:\n",
    "                review_paragraph = str(review.p).strip()[1:-1]\n",
    "            \n",
    "            # 2. find the review rating, if it exists, if not, ignore\n",
    "            sentiment = None\n",
    "            if review.find(\"div\", {\"class\":\"ddc-rating-summary\"}):\n",
    "                rating = int(review.find(\"div\", {\"class\":\"ddc-rating-summary\"}).span.b.string)\n",
    "                if rating < 5:\n",
    "                    sentiment = 'pos'\n",
    "                else:\n",
    "                    sentiment = 'neg'\n",
    "            else:\n",
    "                continue \n",
    "    \n",
    "            reviews.append(list([date, review_paragraph, sentiment]))\n",
    "            \n",
    "        # go to next page if not on the last page\n",
    "        if review_page_content.find_all(\"li\",class_='ddc-paging-item-next') is None:\n",
    "            isHaveNextPage=False\n",
    "        page += 1\n",
    "    print(\"Done with \", drug_name)\n",
    "    return reviews\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reviews = main()\n",
    "    reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d06ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9646073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
